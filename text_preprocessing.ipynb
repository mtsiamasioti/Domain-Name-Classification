{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "474b7cae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T10:18:20.941524Z",
     "start_time": "2022-06-12T10:18:20.935099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a83a4daa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T10:18:24.429622Z",
     "start_time": "2022-06-12T10:18:20.942423Z"
    }
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import csv\n",
    "import re\n",
    "import zipfile\n",
    "import pickle\n",
    "import os\n",
    "from io import BytesIO\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import spacy\n",
    "#python -m spacy download el_core_news_sm -- rin in cmd\n",
    "nlp = spacy.load(\"el_core_news_sm\")\n",
    "nlp.max_length = 3000000\n",
    "\n",
    "import nltk\n",
    "#nltk.download(\"punkt\")\n",
    "import string\n",
    "from nltk import WhitespaceTokenizer\n",
    "\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7781002e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T14:33:54.783173Z",
     "start_time": "2022-06-12T14:33:54.778895Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read training data\n",
    "train_domains = list()\n",
    "y_train = list()\n",
    "with open(\"train.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        l = line.split(\",\")\n",
    "        train_domains.append(l[0])\n",
    "        y_train.append(l[1][:-1])\n",
    "\n",
    "# Read test data\n",
    "test_domains = list()\n",
    "with open(\"test.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        l = line.split(\",\")\n",
    "        test_domains.append(l[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096aa1e8",
   "metadata": {},
   "source": [
    "## Text Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c600a7df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T10:18:24.450664Z",
     "start_time": "2022-06-12T10:18:24.441215Z"
    }
   },
   "outputs": [],
   "source": [
    "def regex(word):\n",
    "    '''function that removes punctuation and digits from a sequence of words and replaces them with space'''\n",
    "    #website = [re.sub('^www\\.', '', urlparse(i).netloc) for i in wordseq] #keep website\n",
    "    word = re.sub(r'\\S*https?:\\S*',' ', word)  #removes website\n",
    "    word = re.sub(r'\\W',' ', word)  #removes punctuation\n",
    "    word = re.sub(r'\\d',' ', word)  #removes digits\n",
    "    word = re.sub(r\"[^A-Ωα-ωάΆώίέήό]\", \" \", word)\n",
    "    word = re.sub(r\"\\s{2,}\", \" \", word)\n",
    "    \n",
    "    return word\n",
    "\n",
    "def text_cleanup(tokens):\n",
    "    ''' function that applies all cleaning up methods in text'''\n",
    "    #GREEK STOPWORDS - list\n",
    "    greek_stopwords = pd.read_csv('http://archive.aueb.gr:8085/files/stopwords.txt', \n",
    "                                  header=None, sep='\\t', names=['stop'])\n",
    "    greek_stopwords = greek_stopwords.stop.values.tolist()\n",
    "\n",
    "    #replace \\n, *#* characters with space and lower all letters\n",
    "    tokens = tokens.replace(\"\\n\",\" \").replace(\"*#*\",\" \").lower()\n",
    "    \n",
    "    #remove punctuation and digits\n",
    "    tokens = regex(tokens)\n",
    "    \n",
    "    #list of words, seperated by whitespaces:\n",
    "    whitespace_tk = WhitespaceTokenizer()\n",
    "    tokens = whitespace_tk.tokenize(tokens)\n",
    "    \n",
    "    #remove stopwords\n",
    "    tokens = [word for word in tokens if word not in greek_stopwords and len(word) > 2 and len(word) < 10] \n",
    "\n",
    "    #add lemmatizer\n",
    "    nlp = spacy.load(\"el_core_news_sm\")\n",
    "    nlp.max_length = len(' '.join(tokens)) + 100\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    tokens = [token.lemma_ if token.lemma_ != \"-PRON-\" else token for token in doc]\n",
    "    \n",
    "    tokens = [word for word in tokens if len(word) > 2 and len(word) < 10]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b308350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T10:18:55.215286Z",
     "start_time": "2022-06-12T10:18:24.451971Z"
    }
   },
   "outputs": [],
   "source": [
    "# TEXT DATA\n",
    "# Read textual content of webpages of domain names\n",
    "text = dict()\n",
    "with zipfile.ZipFile(\"domains.zip\", \"r\") as zfile:\n",
    "    for filename in zfile.namelist():\n",
    "        if re.search(r\"\\.zip$\", filename) is not None:\n",
    "            zfiledata = BytesIO(zfile.read(filename))\n",
    "            with zipfile.ZipFile(zfiledata) as zfile2:\n",
    "                text[filename[:-4]] = \"\"\n",
    "                for name2 in zfile2.namelist():\n",
    "                    file = zfile2.read(name2)\n",
    "                    text[filename[:-4]] += file.decode(\"utf16\") + \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871128fa",
   "metadata": {},
   "source": [
    "#### Text train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b16f663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T10:18:55.219812Z",
     "start_time": "2022-06-12T10:18:55.216661Z"
    }
   },
   "outputs": [],
   "source": [
    "#katharismos\n",
    "def get_text(domains):\n",
    "    data = list()\n",
    "    empty = 0\n",
    "    for domain in domains:\n",
    "        try:\n",
    "            data.append(text_cleanup(text[domain]))\n",
    "        except KeyError:\n",
    "            empty += 1\n",
    "            data.append(['empty_str'])\n",
    "    print('No text:', empty)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18afe158",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T10:24:39.960028Z",
     "start_time": "2022-06-12T10:18:55.221203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No text: 10\n",
      "No text: 12\n",
      "No text: 18\n",
      "No text: 19\n",
      "No text: 15\n",
      "No text: 20\n",
      "No text: 20\n",
      "No text: 17\n",
      "No text: 23\n"
     ]
    }
   ],
   "source": [
    "results_train = Parallel(n_jobs=6)(delayed(get_text)(train_domains[site: site+120]) for site in range(0,len(train_domains),120))\n",
    "results_test = Parallel(n_jobs=6)(delayed(get_text)(test_domains[site: site+120]) for site in range(0,len(test_domains),120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d004016",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T10:24:39.972794Z",
     "start_time": "2022-06-12T10:24:39.961335Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = [item for sublist in results_train for item in sublist]\n",
    "test_data = [item for sublist in results_test for item in sublist]\n",
    "\n",
    "#create dataframes\n",
    "train = pd.DataFrame({'domain': train_domains, 'text':train_data})\n",
    "test = pd.DataFrame({'domain': test_domains, 'text':test_data})\n",
    "all_text = pd.concat([train,test], axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "428c80a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T10:24:39.985849Z",
     "start_time": "2022-06-12T10:24:39.973990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>domain</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>kollintzas.gr</td>\n",
       "      <td>[καταβολή, διδάκτρων, συνάντηση, δήλωση, δωρεά...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>naxos.gr</td>\n",
       "      <td>[συνάντηση, μουσικός, παίζω, τσαμπος, τουμπάκι...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>auth.gr</td>\n",
       "      <td>[πρόσκληση, εκδήλωση, τμήμα, πρόσκληση, εκδήλω...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>kappoutel.gr</td>\n",
       "      <td>[προ, όντα, έχω, πρόσφατα, προσφορά, προ, όντα...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ilioupoli.gr</td>\n",
       "      <td>[ηλιουπολη, φίλες, φίλος, χριστος, γεννα, μήνυ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index         domain                                               text\n",
       "0      0  kollintzas.gr  [καταβολή, διδάκτρων, συνάντηση, δήλωση, δωρεά...\n",
       "1      1       naxos.gr  [συνάντηση, μουσικός, παίζω, τσαμπος, τουμπάκι...\n",
       "2      2        auth.gr  [πρόσκληση, εκδήλωση, τμήμα, πρόσκληση, εκδήλω...\n",
       "3      3   kappoutel.gr  [προ, όντα, έχω, πρόσφατα, προσφορά, προ, όντα...\n",
       "4      4   ilioupoli.gr  [ηλιουπολη, φίλες, φίλος, χριστος, γεννα, μήνυ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49cdb4b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T10:24:40.979165Z",
     "start_time": "2022-06-12T10:24:39.986741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of term \"εισαγωγή\": 288\n",
      "Size of the vocabulary: 187571\n"
     ]
    }
   ],
   "source": [
    "# Extract vocabulary \n",
    "# Map words to integers from 1,...,size of vocabulary\n",
    "vocab = dict()\n",
    "for doc in train_data:\n",
    "    for term in doc:\n",
    "        if term not in vocab:\n",
    "            vocab[term] = len(vocab)+1\n",
    "\n",
    "for doc in test_data:\n",
    "    for term in doc:\n",
    "        if term not in vocab:\n",
    "            vocab[term] = len(vocab)+1\n",
    "\n",
    "print('Index of term \"εισαγωγή\":', vocab['εισαγωγή'])\n",
    "print(\"Size of the vocabulary:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b353c",
   "metadata": {},
   "source": [
    "#### Doc2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f84df",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-06-12T14:34:52.394Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(all_text.text[i], [all_text.domain[i]]) for i in range(0,len(all_text))]\n",
    "model = Doc2Vec(documents, vector_size=32, window=4, dm=1, hs=0, min_count=2, workers=8, seed=123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b4383a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-06-12T14:34:53.658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create train matrix and test matrices\n",
    "X_train_d2v = np.zeros((len(train_domains), 32))\n",
    "for i,domain in enumerate(train_domains):\n",
    "    X_train_d2v[i,:] = model.docvecs[domain]\n",
    "    \n",
    "# Split training matrix (and labels) into a training and a validation matrix (and labels)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=123)\n",
    "\n",
    "X_test_d2v = np.zeros((len(test_domains), 32))\n",
    "for i,domain in enumerate(test_domains):\n",
    "    X_test_d2v[i,:] = model.docvecs[domain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5daaefd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-06-12T14:34:54.233Z"
    }
   },
   "outputs": [],
   "source": [
    "#SAVE RESULTS\n",
    "with open('X_train_text_d2v', 'wb') as f:\n",
    "    pickle.dump(X_train_d2v, f)\n",
    "    \n",
    "with open('X_test_text_d2v', 'wb') as f:\n",
    "    pickle.dump(X_test_d2v, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8553f332",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T08:53:36.347403Z",
     "start_time": "2022-06-09T08:53:36.344236Z"
    }
   },
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4529b5f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T10:24:59.508643Z",
     "start_time": "2022-06-12T10:24:59.504852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average size of training data 4541.723370429253\n",
      "Average size of test data 4127.020109689214\n",
      "Length of longest document: 359293\n"
     ]
    }
   ],
   "source": [
    "# Compute average length of documents\n",
    "mean_size_train = np.mean([len(doc) for doc in train_data])\n",
    "mean_size_test = np.mean([len(doc) for doc in test_data])\n",
    "print('Average size of training data', mean_size_train)\n",
    "print('Average size of test data', mean_size_test)\n",
    "\n",
    "# Compute longest documents\n",
    "max_size_train = np.max([len(doc) for doc in train_data])\n",
    "max_size_test = np.max([len(doc) for doc in test_data])\n",
    "max_size = max(max_size_train, max_size_test)\n",
    "\n",
    "print('Length of longest document:', max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "130eb86e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T10:24:59.978890Z",
     "start_time": "2022-06-12T10:24:59.509721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix dimensionality:  (1258, 1000)\n",
      "Test matrix dimensionality:  (547, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Set maximum length equal to 1000\n",
    "max_size = 1000\n",
    "# Create training matrix - label encoding\n",
    "X_train = np.zeros((len(train_data), max_size))\n",
    "for i,doc in enumerate(train_data):\n",
    "    for j,word in enumerate(doc):\n",
    "        if j == max_size:\n",
    "            break\n",
    "        X_train[i,j] = vocab[word]\n",
    "\n",
    "# Create test matrix\n",
    "X_test = np.zeros((len(test_data), max_size))\n",
    "for i,doc in enumerate(test_data):\n",
    "    for j,word in enumerate(doc):\n",
    "        if j == max_size:\n",
    "            break\n",
    "        X_test[i,j] = vocab[word]\n",
    "\n",
    "print(\"Train matrix dimensionality: \", X_train.shape)\n",
    "#print(\"Validation matrix dimensionality: \", X_val.shape)\n",
    "print(\"Test matrix dimensionality: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "969f2b07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T10:25:00.047685Z",
     "start_time": "2022-06-12T10:24:59.980032Z"
    }
   },
   "outputs": [],
   "source": [
    "#Save cleaned data to pickle file\n",
    "with open('X_train_text', 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "\n",
    "with open('X_test_text', 'wb') as f:\n",
    "    pickle.dump(X_test, f)\n",
    "    \n",
    "with open('vocab', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c4acd1",
   "metadata": {},
   "source": [
    "### Tune Classifier with Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2a76e87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T14:34:29.798757Z",
     "start_time": "2022-06-12T14:34:29.795757Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split training matrix (and labels) into a training and a validation matrix (and labels)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_d2v, y_train, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3937c457",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T14:34:35.727469Z",
     "start_time": "2022-06-12T14:34:30.313851Z"
    }
   },
   "outputs": [],
   "source": [
    "#GRID SEARCH\n",
    "clf = LogisticRegression(max_iter=10000)\n",
    "parameters = [{'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']},\n",
    "              {'penalty':['none', 'l2']},\n",
    "              {'C':[0.001, 0.01, 0.1, 1, 10, 100]}]\n",
    "\n",
    "grid_search = GridSearchCV(estimator = clf,  \n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'f1_weighted',\n",
    "                           cv = 5,\n",
    "                           verbose=0)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "y_pred = grid_search.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b18e197",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T14:34:35.736077Z",
     "start_time": "2022-06-12T14:34:35.728746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.70      0.76        64\n",
      "           1       0.83      0.68      0.75        28\n",
      "           2       0.68      0.57      0.62        37\n",
      "           3       0.61      0.86      0.72        79\n",
      "           4       0.67      0.55      0.60        11\n",
      "           5       0.50      0.60      0.55         5\n",
      "           6       0.57      0.31      0.40        13\n",
      "           7       0.33      0.50      0.40         4\n",
      "           8       1.00      0.20      0.33         5\n",
      "           9       0.25      0.17      0.20         6\n",
      "\n",
      "    accuracy                           0.67       252\n",
      "   macro avg       0.63      0.51      0.53       252\n",
      "weighted avg       0.69      0.67      0.67       252\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[45,  1,  2, 14,  0,  0,  1,  0,  0,  1],\n",
       "       [ 1, 19,  0,  8,  0,  0,  0,  0,  0,  0],\n",
       "       [ 5,  0, 21,  7,  1,  0,  0,  2,  0,  1],\n",
       "       [ 1,  2,  5, 68,  0,  1,  1,  1,  0,  0],\n",
       "       [ 0,  0,  1,  3,  6,  0,  1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  1,  1,  3,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  4,  1,  2,  4,  0,  0,  1],\n",
       "       [ 0,  0,  0,  2,  0,  0,  0,  2,  0,  0],\n",
       "       [ 0,  0,  1,  2,  0,  0,  0,  1,  1,  0],\n",
       "       [ 1,  1,  1,  2,  0,  0,  0,  0,  0,  1]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(classification_report(np.array(list(map(int, y_val))), np.argmax(y_pred, axis=1)))\n",
    "confusion_matrix(np.array(list(map(int, y_val))), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f340a9",
   "metadata": {},
   "source": [
    "### Final Classifier, Score: 1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41f8c0a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T10:25:16.356326Z",
     "start_time": "2022-06-12T10:25:16.353204Z"
    }
   },
   "outputs": [],
   "source": [
    "all_train = np.vstack((X_train, X_val))\n",
    "all_y = np.concatenate((y_train,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d0d4ac0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T10:25:16.473846Z",
     "start_time": "2022-06-12T10:25:16.357810Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use logistic regression to classify the webpages of the test set\n",
    "clf = LogisticRegression(**grid_search.best_params_, max_iter=10000)\n",
    "clf.fit(all_train, all_y)\n",
    "y_pred = clf.predict_proba(X_test_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93ca5029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T10:25:16.479557Z",
     "start_time": "2022-06-12T10:25:16.475249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 2, 1, 3, 6, 3, 0, 6, 3, 1, 1, 3, 0, 0, 2, 0, 3, 0, 4, 3, 0, 0,\n",
       "       9, 3, 4, 3, 1, 1, 3, 3, 1, 2, 0, 2, 3, 3, 3, 3, 0, 2, 2, 6, 3, 0,\n",
       "       3, 0, 2, 1, 0, 0, 3, 0, 1, 3, 0, 3, 3, 1, 0, 2, 1, 0, 4, 2, 0, 9,\n",
       "       3, 3, 3, 0, 3, 0, 1, 2, 3, 1, 4, 2, 3, 7, 2, 0, 2, 4, 9, 0, 3, 0,\n",
       "       6, 0, 0, 0, 0, 5, 0, 0, 4, 4, 0, 3, 2, 3, 4, 3, 3, 2, 3, 5, 5, 5,\n",
       "       2, 4, 2, 3, 0, 0, 0, 0, 0, 2, 3, 0, 3, 7, 0, 1, 3, 9, 2, 2, 2, 3,\n",
       "       0, 2, 8, 3, 2, 3, 1, 3, 6, 3, 3, 0, 4, 2, 3, 3, 1, 1, 1, 1, 3, 3,\n",
       "       3, 5, 3, 0, 3, 4, 2, 0, 3, 3, 3, 3, 3, 2, 1, 3, 0, 7, 3, 0, 0, 5,\n",
       "       0, 3, 0, 3, 0, 3, 0, 2, 7, 0, 3, 0, 3, 5, 3, 6, 9, 1, 2, 3, 4, 0,\n",
       "       2, 2, 6, 0, 5, 3, 3, 3, 3, 2, 3, 0, 0, 7, 3, 2, 2, 4, 3, 0, 3, 7,\n",
       "       3, 0, 3, 0, 2, 0, 0, 3, 0, 1, 3, 0, 0, 0, 3, 2, 3, 7, 0, 3, 3, 0,\n",
       "       1, 1, 0, 0, 3, 3, 2, 0, 0, 0, 2, 0, 3, 0, 4, 2, 2, 3, 0, 3, 0, 3,\n",
       "       3, 3, 1, 3, 3, 2, 3, 3, 2, 0, 1, 3, 0, 1, 1, 3, 1, 3, 0, 3, 3, 0,\n",
       "       1, 3, 2, 0, 2, 2, 4, 3, 5, 0, 3, 3, 0, 3, 0, 3, 3, 9, 3, 0, 5, 0,\n",
       "       3, 3, 2, 3, 0, 0, 1, 3, 3, 1, 1, 0, 3, 1, 0, 3, 3, 3, 3, 2, 3, 2,\n",
       "       1, 6, 0, 0, 4, 0, 3, 3, 3, 0, 3, 1, 2, 3, 0, 3, 3, 2, 3, 5, 3, 0,\n",
       "       6, 2, 3, 1, 3, 3, 1, 3, 3, 0, 3, 4, 2, 5, 0, 3, 0, 0, 1, 3, 3, 0,\n",
       "       3, 0, 4, 1, 3, 3, 3, 0, 9, 0, 2, 6, 3, 3, 3, 8, 1, 0, 3, 1, 0, 5,\n",
       "       3, 2, 3, 0, 2, 6, 3, 3, 0, 8, 3, 1, 2, 4, 1, 3, 3, 3, 2, 3, 3, 3,\n",
       "       3, 2, 0, 3, 7, 3, 5, 0, 1, 3, 2, 1, 2, 3, 5, 2, 1, 0, 3, 3, 3, 2,\n",
       "       3, 6, 6, 3, 3, 3, 0, 0, 0, 1, 0, 3, 3, 3, 4, 3, 2, 2, 0, 1, 3, 3,\n",
       "       0, 5, 1, 3, 3, 2, 0, 5, 3, 3, 3, 1, 3, 2, 5, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 0, 3, 0, 3, 1, 3, 6, 2, 0, 0, 3, 3, 7, 5, 3, 3, 3, 3, 3, 0,\n",
       "       3, 0, 3, 3, 2, 3, 3, 3, 0, 3, 3, 0, 6, 5, 0, 0, 3, 3, 3, 0, 3, 3,\n",
       "       1, 3, 1, 0, 3, 0, 3, 2, 8, 0, 9, 3, 1, 4, 3, 3, 0, 0, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4be4d3a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T10:25:16.498303Z",
     "start_time": "2022-06-12T10:25:16.480872Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write predictions to a file\n",
    "with open('sample_submission.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    lst = list()\n",
    "    for i in range(10):\n",
    "        lst.append('class_'+str(i))\n",
    "    lst.insert(0, \"domain_name\")\n",
    "    writer.writerow(lst)\n",
    "    for i,test_host in enumerate(test_domains):\n",
    "        lst = y_pred[i,:].tolist()\n",
    "        lst.insert(0, test_host)\n",
    "        writer.writerow(lst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal",
   "language": "python",
   "name": "causal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
